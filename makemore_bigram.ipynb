{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c1347d-642f-4578-8313-ecdc6d09eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5ba766-968c-4bd8-a592-20472cdb133e",
   "metadata": {},
   "source": [
    "### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a141c491-d833-4f7a-97a3-7f5976e51812",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7045acfd-1296-443d-a5b2-91b9819f5435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dbfbc08-bc05-4bb5-a79e-ac9918a229b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88acf479-f5d1-405c-a031-05eeb3556941",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8deff26b-0973-428f-998e-841038736f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5831040-3469-4ccc-b376-39243a25d6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7686190605163574\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(1):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  #loss = -probs[torch.arange(num), ys].log().mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f9e6385-aff9-432b-909f-1660a5dd93ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texzmkloglquszipczktxhkmpmzistttwinmlgdukzka.\n",
      "zr.\n",
      "rocxtpucjwtsc.\n",
      "gmtokmxczisqytxugkwpt.\n",
      "dajkkluydjmscdgu.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "      \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e765b96-0101-4b84-ab30-a975343be69a",
   "metadata": {},
   "source": [
    "### E01 Trigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b05ee-c1ab-46bb-8641-1e185988db35",
   "metadata": {},
   "source": [
    "Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c008505c-b4c4-4b2f-b71e-e15dad633c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chs = ['.']*2 + list(\"emma\") + ['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29c0dfc9-e34a-44be-8dc8-24fd7a1815bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ch1='.', ch2='.', ch3='e'\n",
      "ch1='.', ch2='e', ch3='m'\n",
      "ch1='e', ch2='m', ch3='m'\n",
      "ch1='m', ch2='m', ch3='a'\n",
      "ch1='m', ch2='a', ch3='.'\n"
     ]
    }
   ],
   "source": [
    "for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    print(f'{ch1=}, {ch2=}, {ch3=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b69275-9a26-4891-945c-36db407e3a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.']*2 + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = [stoi[ch1], stoi[ch2]]\n",
    "    ix2 = stoi[ch3]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.shape[0]\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((54, 27), generator=g, requires_grad=True) # X will be N x (27 * 2) = N x 54, so we need 54 neurons, giving 27 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f993138-eeb2-435e-b2ed-1ce190baad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3549153804779053\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(1):\n",
    "  \n",
    "  # forward pass\n",
    "  x1 = F.one_hot(xs[:,0], 27).float()    # (N,27)\n",
    "  x2 = F.one_hot(xs[:,1], 27).float()    # (N,27)\n",
    "  xenc = torch.cat([x1, x2], dim=1)      # (N,54)\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac3fd79b-0fcf-4b34-834c-d4c9cf48a00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze.\n",
      "morlyurailaziaydamellimittain.\n",
      "lusan.\n",
      "ka.\n",
      "da.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  out = []\n",
    "  ix = [0, 0]\n",
    "  while True:\n",
    "    x1 = F.one_hot(torch.tensor(ix[0]), 27).float()    # (1,27)\n",
    "    x2 = F.one_hot(torch.tensor(ix[1]), 27).float()    # (1,27)\n",
    "    xenc = torch.cat([x1, x2]).reshape(1, 54)    # (1,54)\n",
    "    # xenc = F.one_hot(torch.tensor(ix.reshape((-1, 54))), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "      \n",
    "    ix = [ix[1], torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()]\n",
    "    out.append(itos[ix[1]])\n",
    "    if ix[1] == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ad28fb-9078-4205-b864-004d1df018e1",
   "metadata": {},
   "source": [
    "### E02 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e59c44b-c744-41af-87e0-c566cb61b637",
   "metadata": {},
   "source": [
    "Split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb27d1-3b54-49b8-8ddb-83d8c05f877a",
   "metadata": {},
   "source": [
    "#### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f015f801-7bb3-44de-91c1-058639e81c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "train_xs, train_ys = xs[:int(0.8 * num)], ys[:int(0.8 * num)]\n",
    "dev_xs, dev_ys = xs[int(0.8 * num):int(0.9 * num)], ys[int(0.8 * num):int(0.9 * num)]\n",
    "test_xs, test_ys = xs[int(0.9 * num):], ys[int(0.9 * num):]\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6da9e28c-4e86-4187-981d-0b95ca000f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.452766180038452\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(1):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(train_xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(train_xs.nelement()), train_ys].log().mean() + 0.01*(W**2).mean()\n",
    "  #loss = -probs[torch.arange(num), ys].log().mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ef39ef8-c42d-41b0-a17d-c16964d2186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_calc_loss(xs, ys, W):\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(xs.nelement()), ys].log().mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9087a193-6761-49a3-be5b-8dcb1cb1b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Model Dev loss: 2.5989654064178467\n",
      "Bigram Model Test loss: 2.602977752685547\n"
     ]
    }
   ],
   "source": [
    "print(f\"Bigram Model Dev loss: {bigram_calc_loss(dev_xs, dev_ys, W).item()}\")\n",
    "print(f\"Bigram Model Test loss: {bigram_calc_loss(test_xs, test_ys, W).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0c706-5811-4024-bd66-c33ee5023934",
   "metadata": {},
   "source": [
    "#### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62363aef-71ff-4ca4-92d4-0be9937b1516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.']*2 + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = [stoi[ch1], stoi[ch2]]\n",
    "    ix2 = stoi[ch3]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.shape[0]\n",
    "print('number of examples: ', num)\n",
    "\n",
    "train_xs, train_ys = xs[:int(0.8 * num)], ys[:int(0.8 * num)]\n",
    "dev_xs, dev_ys = xs[int(0.8 * num):int(0.9 * num)], ys[int(0.8 * num):int(0.9 * num)]\n",
    "test_xs, test_ys = xs[int(0.9 * num):], ys[int(0.9 * num):]\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((54, 27), generator=g, requires_grad=True) # X will be N x (27 * 2) = N x 54, so we need 54 neurons, giving 27 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f91985f-1622-4e87-9b9f-0ad3eda50320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3210928440093994\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(1):\n",
    "  \n",
    "  # forward pass\n",
    "  x1 = F.one_hot(train_xs[:,0], 27).float()    # (N,27)\n",
    "  x2 = F.one_hot(train_xs[:,1], 27).float()    # (N,27)\n",
    "  xenc = torch.cat([x1, x2], dim=1)      # (N,54)\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(train_xs.shape[0]), train_ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "36357ac7-9426-4a1b-b6cd-5afd4acc6bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_calc_loss(xs, ys, W):\n",
    "    x1 = F.one_hot(xs[:,0], 27).float()\n",
    "    x2 = F.one_hot(xs[:,1], 27).float()\n",
    "    xenc = torch.cat([x1, x2], dim=1)\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(xs.shape[0]), ys].log().mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a9c1f9aa-d7e4-4ed5-87c8-fcfbab5f7803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Model Dev loss: 2.5064163208007812\n",
      "Trigram Model Test loss: 2.515704870223999\n"
     ]
    }
   ],
   "source": [
    "print(f\"Trigram Model Dev loss: {trigram_calc_loss(dev_xs, dev_ys, W).item()}\")\n",
    "print(f\"Trigram Model Test loss: {trigram_calc_loss(test_xs, test_ys, W).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cef8e-0070-411e-95a5-94dc160d1d70",
   "metadata": {},
   "source": [
    "### E03 Tune Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2086fbef-47aa-4c89-9d66-5c6df5657324",
   "metadata": {},
   "source": [
    "Use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11590339-b1e4-4fa0-8710-777490e10d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Loss: 2.921194314956665\n",
      "Dev Loss: 2.6151700019836426\n",
      "Dev Loss: 2.5602316856384277\n",
      "Dev Loss: 2.548191785812378\n",
      "Dev Loss: 2.5527312755584717\n",
      "Dev Loss: 2.559356212615967\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "Ws = []\n",
    "regs = [10, 1, 0.1, 0.01, 0.001, 0]\n",
    "dev_losses = []\n",
    "for reg in regs:\n",
    "    W = torch.randn((54, 27), generator=g, requires_grad=True)\n",
    "    for k in range(500):\n",
    "      # forward pass\n",
    "      x1 = F.one_hot(train_xs[:,0], 27).float()    # (N,27)\n",
    "      x2 = F.one_hot(train_xs[:,1], 27).float()    # (N,27)\n",
    "      xenc = torch.cat([x1, x2], dim=1)      # (N,54)\n",
    "      logits = xenc @ W # predict log-counts\n",
    "      counts = logits.exp() # counts, equivalent to N\n",
    "      probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "      loss = -probs[torch.arange(train_xs.shape[0]), train_ys].log().mean() + reg*(W**2).mean()\n",
    "      # print(loss.item())\n",
    "      \n",
    "      # backward pass\n",
    "      W.grad = None # set to zero the gradient\n",
    "      loss.backward()\n",
    "      \n",
    "      # update\n",
    "      W.data += -5 * W.grad\n",
    "    \n",
    "    dev_loss = trigram_calc_loss(dev_xs, dev_ys, W).item()\n",
    "    print(f\"Dev Loss: {dev_loss}\")\n",
    "    dev_losses.append(dev_loss)\n",
    "    \n",
    "    Ws.append(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f270944e-d601-4b27-a6a8-d9c0557b7ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.921194314956665,\n",
       " 2.6151700019836426,\n",
       " 2.5602316856384277,\n",
       " 2.548191785812378,\n",
       " 2.5527312755584717,\n",
       " 2.559356212615967]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_losses # U curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cd9eafcc-8555-46a8-91c8-6771acea6f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_dev_loss=2.548191785812378 at index 3, best_reg=0.1\n"
     ]
    }
   ],
   "source": [
    "best_dev_loss = min(dev_losses)\n",
    "best_dev_index = np.argmin(dev_losses).item()\n",
    "best_reg = regs[best_dev_index]\n",
    "print(f\"{best_dev_loss=} at index {best_dev_index}, {best_reg=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "63c8df14-7069-4e6c-a3f1-07b6c313527e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5554730892181396"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_calc_loss(test_xs, test_ys, Ws[best_dev_index]).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f349e4-796a-4721-beda-6ad25645d7ad",
   "metadata": {},
   "source": [
    "### E04 One-hot selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527388a6-9430-43b7-bfc5-9d99d46f8b38",
   "metadata": {},
   "source": [
    "We saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "278495f0-80f7-4cfd-b090-513e710853af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_calc_loss(xs, ys, W):\n",
    "    x1 = F.one_hot(xs[:,0], 27).float()\n",
    "    x2 = F.one_hot(xs[:,1], 27).float()\n",
    "    xenc = torch.cat([x1, x2], dim=1)\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(xs.shape[0]), ys].log().mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2904526d-acf8-490e-ab90-1345e0911a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_calc_loss_opt(xs, ys, W):\n",
    "    logits = W[xs[:, 0]] + W[27 + xs[:, 1]] # indexing\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(xs.shape[0]), ys].log().mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3a1225a5-33ea-4e50-8d45-8ae8bf692b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5643, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_calc_loss(test_xs, test_ys, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "140803a8-3fac-4928-a5e1-cd36f4e9e87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5643, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_calc_loss_opt(test_xs, test_ys, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9142564e-caf3-4629-ae1f-4db99e94bed7",
   "metadata": {},
   "source": [
    "### E05 Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026505d3-33ad-4066-9a16-77e847c349b8",
   "metadata": {},
   "source": [
    "Look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fd035888-e0b2-41b0-8200-1a7d3ee78839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((54, 27), generator=g, requires_grad=True) # X will be N x (27 * 2) = N x 54, so we need 54 neurons, giving 27 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0bf27f90-2e8b-4cb6-a9f5-8e4c66a4ded6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.334840774536133\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(1):\n",
    "  \n",
    "  # forward pass\n",
    "  x1 = F.one_hot(train_xs[:,0], 27).float()    # (N,27)\n",
    "  x2 = F.one_hot(train_xs[:,1], 27).float()    # (N,27)\n",
    "  xenc = torch.cat([x1, x2], dim=1)      # (N,54)\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  # loss = -probs[torch.arange(train_xs.shape[0]), train_ys].log().mean() + 0.01*(W**2).mean()\n",
    "  loss = F.cross_entropy(logits, train_ys) + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "\n",
    "  # update\n",
    "  W.data += -1 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b9ec07-854c-4768-8eb8-102ee78cf083",
   "metadata": {},
   "source": [
    "Therefore, it is the same as the loss we calculated before. Also more stable and better optimised."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
